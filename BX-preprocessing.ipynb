{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as spsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv('BX-Books.csv.gz', sep='\\\";\\\"', encoding = 'cp1252')\n",
    "ratings = pd.read_csv('BX-Book-Ratings.csv.gz', sep=';', encoding = 'cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_content = pd.read_csv('compiled_books_content.txt.gz', sep='\\t', encoding = 'cp1252', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to collect all books and assign them with sequence numbers.\n",
    "\n",
    "We filter books. We have to make sure a book has metadata and content. We only collect English books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "# the books with metadata.\n",
    "isbn_set = set()\n",
    "for i in range(len(books['\\\"ISBN'])):\n",
    "    isbn = books['\\\"ISBN'][i][1:]\n",
    "    isbn_set.add(isbn)\n",
    "\n",
    "# the books with more detailed content information.\n",
    "book_map = {}\n",
    "num_books = 0\n",
    "for book, abstract in zip(book_content[0], book_content[3]):\n",
    "    if book in book_map or book not in isbn_set:\n",
    "        continue\n",
    "    try:\n",
    "        if detect(abstract) == 'en':\n",
    "            book_map[book] = num_books\n",
    "            num_books += 1\n",
    "    except:\n",
    "        continue\n",
    "assert len(book_map) == num_books\n",
    "print('#books:', num_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all of the metadata of the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_titles = {}\n",
    "book_authors = {}\n",
    "book_years = np.zeros(shape=(num_books))\n",
    "book_publishers = {}\n",
    "book_abstracts = {}\n",
    "\n",
    "for i in range(len(books['\\\"ISBN'])):\n",
    "    isbn = books['\\\"ISBN'][i][1:]\n",
    "    title = books['Book-Title'][i]\n",
    "    author = books['Book-Author'][i]\n",
    "    year = books['Year-Of-Publication'][i]\n",
    "    publisher = books['Publisher'][i]\n",
    "    if isbn not in book_map:\n",
    "        continue\n",
    "    book_idx = book_map[isbn]\n",
    "    book_titles[book_idx] = title\n",
    "    book_authors[book_idx] = author\n",
    "    book_years[book_idx] = year\n",
    "    book_publishers[book_idx] = publisher\n",
    "print(len(book_titles))\n",
    "    \n",
    "for isbn, title, abstract in zip(book_content[0], book_content[2], book_content[3]):\n",
    "    if isbn in book_map:\n",
    "        idx = book_map[isbn]\n",
    "        book_abstracts[idx] = abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the ratings on the books with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ratings = []\n",
    "for user, isbn, rate in zip(ratings['User-ID'], ratings['ISBN'], ratings['Book-Rating']):\n",
    "    if isbn in book_map:\n",
    "        filter_ratings.append((user, isbn, rate))\n",
    "        \n",
    "print(len(filter_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all users that read books with metadata. The users are assigned with sequence numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map = {}\n",
    "num_users = 0\n",
    "for user, _, _ in filter_ratings:\n",
    "    if user not in user_map:\n",
    "        user_map[user] = num_users\n",
    "        num_users += 1\n",
    "assert len(user_map) == num_users\n",
    "print('#users:', num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a sparse matrix for the user-book interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_arr = np.array([user_map[user] for user, _, _ in filter_ratings], dtype=np.int64)\n",
    "book_arr = np.array([book_map[isbn] for _, isbn, _ in filter_ratings], dtype=np.int64)\n",
    "rate_arr = np.array([rate for _, _, rate in filter_ratings], dtype=np.int64)\n",
    "\n",
    "user_book_spm = spsp.coo_matrix((np.ones((len(user_arr))), (user_arr, book_arr)))\n",
    "user_book_ratings = spsp.coo_matrix((rate_arr, (user_arr, book_arr)))\n",
    "print(user_book_spm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the users read less two books. In this case, we cannot use them in testing or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_deg = user_book_spm.dot(np.ones((num_books)))\n",
    "print(np.sum(user_deg <= 2))\n",
    "book_deg = user_book_spm.transpose().dot(np.ones((num_users)))\n",
    "print(np.sum(book_deg <= 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a new mapping between original user id and new id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map1 = {}\n",
    "num_users = 0\n",
    "for user, _, _ in filter_ratings:\n",
    "    orig_idx = user_map[user]\n",
    "    if user not in user_map1 and user_deg[orig_idx] > 2:\n",
    "        user_map1[user] = num_users\n",
    "        num_users += 1\n",
    "assert len(user_map1) == num_users\n",
    "print('#users:', num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_book_spm = user_book_spm.tocsr()[user_deg > 2]\n",
    "user_book_ratings = user_book_ratings.tocsr()[user_deg > 2]\n",
    "print(user_book_spm.shape)\n",
    "\n",
    "user_deg = user_book_spm.dot(np.ones((num_books)))\n",
    "print(np.sum(user_deg <= 2))\n",
    "book_deg = user_book_spm.transpose().dot(np.ones((num_users)))\n",
    "print(np.sum(book_deg <= 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts(book_attributes):\n",
    "    popularity = {}\n",
    "    for _, author in book_attributes.items():\n",
    "        if author in popularity:\n",
    "            popularity[author] += 1\n",
    "        else:\n",
    "            popularity[author] = 1\n",
    "    print('#attributes:', len(popularity))\n",
    "    print(np.max([p for _, p in popularity.items()]))\n",
    "    \n",
    "counts(book_authors)\n",
    "counts(book_publishers)\n",
    "uniq_years, year_cnts = np.unique(book_years, return_counts=True)\n",
    "print('#years:', len(uniq_years))\n",
    "print('max #books a year:', np.max(year_cnts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn, Block\n",
    "import gluonnlp as nlp\n",
    "import time\n",
    "import random\n",
    "from gluonnlp.data import BERTTokenizer\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)\n",
    "\n",
    "dropout_prob = 0.1\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "bert_model, bert_vocab = nlp.model.get_model(name='bert_12_768_12',\n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True,\n",
    "                                             ctx=ctx,\n",
    "                                             use_pooler=True,\n",
    "                                             use_decoder=False,\n",
    "                                             use_classifier=False,\n",
    "                                             dropout=dropout_prob,\n",
    "                                             embed_dropout=dropout_prob)\n",
    "tokenizer = BERTTokenizer(bert_vocab, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstract_emb = mx.nd.zeros(shape=(num_books, 768), ctx=mx.gpu(0))\n",
    "for i in range(num_books):\n",
    "    token_ids = mx.nd.expand_dims(mx.nd.array(bert_vocab[tokenizer(book_abstracts[i])],\n",
    "                                              dtype=np.int32, ctx=mx.gpu(0)), axis=0)\n",
    "    token_types = mx.nd.ones_like(token_ids, ctx=mx.gpu(0))\n",
    "    _, sent_embedding = bert_model(token_ids, token_types)\n",
    "    abstract_emb[i] = sent_embedding.transpose().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_emb = mx.nd.zeros(shape=(num_books, 768), ctx=mx.gpu(0))\n",
    "for i in range(num_books):\n",
    "    token_ids = mx.nd.expand_dims(mx.nd.array(bert_vocab[tokenizer(book_titles[i])],\n",
    "                                              dtype=np.int32, ctx=mx.gpu(0)), axis=0)\n",
    "    token_types = mx.nd.ones_like(token_ids, ctx=mx.gpu(0))\n",
    "    _, sent_embedding = bert_model(token_ids, token_types)\n",
    "    title_emb[i] = sent_embedding.transpose().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abstract_emb.shape)\n",
    "print(title_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bx_book_abstract.npy', abstract_emb.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bx_book_title.npy', title_emb.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_test(user_book_spm):\n",
    "    users = user_book_spm.row\n",
    "    items = user_book_spm.col\n",
    "    picks = np.zeros(shape=(len(users)))\n",
    "    user_book_spm = user_book_spm.tocsr()\n",
    "    indptr = user_book_spm.indptr\n",
    "    valid_set = np.zeros(shape=(num_users))\n",
    "    test_set = np.zeros(shape=(num_users))\n",
    "    for i in range(user_book_spm.shape[0]):\n",
    "        start_idx = indptr[i]\n",
    "        end_idx = indptr[i+1]\n",
    "        idx = np.random.choice(np.arange(start_idx, end_idx), 2, replace=False)\n",
    "        valid_set[i] = items[idx[0]]\n",
    "        picks[idx[0]] = 1\n",
    "        test_set[i] = items[idx[1]]\n",
    "        picks[idx[1]] = 1\n",
    "    users = users[picks == 0]\n",
    "    items = items[picks == 0]\n",
    "    return spsp.coo_matrix((np.ones((len(users),)), (users, items))), valid_set, test_set\n",
    "\n",
    "orig_user_book_spm = user_book_spm.tocsr()\n",
    "user_book_spm, valid_set, test_set = pick_test(user_book_spm.tocoo())\n",
    "print('#training size:', user_book_spm.nnz)\n",
    "users_valid = np.arange(num_users)\n",
    "items_valid = valid_set\n",
    "users_test = np.arange(num_users)\n",
    "items_test = test_set\n",
    "valid_size = len(users_valid)\n",
    "test_size = len(users_test)\n",
    "print('valid set:', valid_size)\n",
    "print('test set:', test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig_user_book_spm.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_neg_set(user_item_spm, neg_sample_size):\n",
    "    num_users = user_item_spm.shape[0]\n",
    "    num_items = user_item_spm.shape[1]\n",
    "    neg_mat = np.zeros(shape=(num_users, neg_sample_size))\n",
    "    for user in range(num_users):\n",
    "        item_set = set()\n",
    "        while len(item_set) < neg_sample_size:\n",
    "            items = np.random.choice(num_items, neg_sample_size, replace=False)\n",
    "            for item in items:\n",
    "                if user_item_spm[user, item] == 0:\n",
    "                    item_set.add(item)\n",
    "                if len(item_set) == neg_sample_size:\n",
    "                    break\n",
    "        neg_mat[user] = np.array(list(item_set))\n",
    "\n",
    "    for user, items in enumerate(neg_mat):\n",
    "        for idx, item in enumerate(items):\n",
    "            assert user_item_spm[user, item] == 0\n",
    "                \n",
    "    return neg_mat\n",
    "\n",
    "neg_valid = gen_neg_set(orig_user_book_spm.tocsr(), 99)\n",
    "neg_test = gen_neg_set(orig_user_book_spm.tocsr(), 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(user_book_spm, open('bx_train.pkl', 'wb'))\n",
    "pickle.dump(abstract_emb, open('bx_book_abstract.pkl', 'wb'))\n",
    "pickle.dump(title_emb, open('bx_book_title.pkl', 'wb'))\n",
    "pickle.dump(user_map1, open('bx_user_map.pkl', 'wb'))\n",
    "pickle.dump(book_map, open('bx_book_map.pkl', 'wb'))\n",
    "pickle.dump((valid_set, test_set), open('bx_eval.pkl', 'wb'))\n",
    "pickle.dump((neg_valid, neg_test), open('bx_neg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
